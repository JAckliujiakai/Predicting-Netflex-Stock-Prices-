# -*- coding: utf-8 -*-
"""Predicting Netflex Stock Prices

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ImqoGdua38rAed8Z49JlMHmgMAatbIRq

# RNN for time-series prediction

Train RNN, LSTM and GRU to predict stock prices.

<img src='https://github.com/RankJay/Deep-Learning-with-Pytorch-from-Facebook-Udacity/blob/master/Recurrent%20Neural%20Networks/time-series/assets/time_prediction.png?raw=true' width=40% />

* 1. loading data
* 2. defining deep learning models in PyTorch
* 3. training and optimizing models
* 4. model evaluation

## Import resources and read data
"""

# import modules
from datetime import datetime
import yfinance as yf
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
import time

import torch
import torch.nn as nn

"""Download data using stock ticker from Yahoo Finance"""

# initialize parameters
start_date = datetime(2011, 11, 1)
end_date = datetime(2023, 11, 1)
stock_ticker= 'NFLX'
# get the data
data = yf.download(stock_ticker, start = start_date,
				end = end_date)

data['Date']=data.index
data.head()

import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style("darkgrid")
plt.figure(figsize = (15,9))
plt.plot(data[['Close']])
plt.title(stock_ticker + " Stock Price",fontsize=18, fontweight='bold')
plt.xlabel('Date',fontsize=18)
plt.ylabel('Close Price (USD)',fontsize=18)
plt.show()

"""## Normalize data"""

price = data[['Close']]
price.info()

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler(feature_range=(-1, 1))
price['Close'] = scaler.fit_transform(price['Close'].values.reshape(-1,1))

def split_data(stock, lookback):
    data_raw = stock.to_numpy() # convert to numpy array
    data = []

    # create a list of time series of window size lookback
    for index in range(len(data_raw) - lookback):
        data.append(data_raw[index: index + lookback])

    data = np.array(data);
    # The last 20% of the data as test set
    test_set_size = int(np.round(0.2*data.shape[0]));
    train_set_size = data.shape[0] - (test_set_size);

    #assign the first lookback -1 time steps as input x and the last time step as target y
    x_train = data[:train_set_size,:-1,:]
    y_train = data[:train_set_size,-1,:]

    x_test = data[train_set_size:,:-1,:]
    y_test = data[train_set_size:,-1,:]

    return [x_train, y_train, x_test, y_test]

lookback = 20 # choose sequence length
x_train, y_train, x_test, y_test = split_data(price, lookback)
print('x_train.shape = ',x_train.shape)
print('y_train.shape = ',y_train.shape)
print('x_test.shape = ',x_test.shape)
print('y_test.shape = ',y_test.shape)

x_train = torch.from_numpy(x_train).type(torch.Tensor)
x_test = torch.from_numpy(x_test).type(torch.Tensor)
y_train = torch.from_numpy(y_train).type(torch.Tensor)
y_test = torch.from_numpy(y_test).type(torch.Tensor)

"""## Define Basic RNN Model"""

class RNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers):
        """
        Initialize the model by setting up the layers.
        """

        super(RNN, self).__init__()

        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)


    def forward(self, x, hidden):
        """
        Perform a forward pass of our model on some input and hidden state.
        """
        # x (batch_size, seq_length, input_dim)
        # hidden (num_layers, batch_size, hidden_dim)
        # r_out (batch_size, time_step, hidden_size)

        # get RNN outputs
        out, hidden = self.rnn(x, hidden)

        return out, hidden

rnn = RNN(32,64,2)
rnn

"""### Check the input and output dimensions


"""

# test that dimensions are as expected
test_rnn = RNN(input_dim=1, output_dim=1, hidden_dim=10, num_layers=2)


# generate evenly spaced, test data pts
test_input= x_train[0:64,:,:] #adds an input_dim dimension

# test_input = test_data.unsqueeze(0) # give it a batch_size of 1 as first dimension
print('Input size: ', test_input.size())

# test out rnn sizes
test_out, test_h = test_rnn(test_input, None)
print('Output size: ', test_out.size())
print('Hidden state size: ', test_h.size())

"""### Training the RNN


"""

# decide on hyperparameters
input_dim=1
output_dim=1
hidden_dim=32
num_layers=2
# instantiate an RNN
rnn = RNN(input_dim, output_dim, hidden_dim, num_layers)
print(rnn)

"""### Loss and Optimization



"""

# MSE loss and Adam optimizer with a learning rate of 0.01
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)

"""#### Start Training

"""

def train(model, x_train, y_train, num_epochs):
  start_time = time.time()
  train_losses = np.zeros(num_epochs)

  for t in range(num_epochs):
      tot_train_loss = 0

      # initialize the hidden state
      hidden = None

      #1. compute outputs from the rnn
      y_train_pred, hidden = model(x_train, hidden)
      #2. calculate the loss
      loss = criterion(y_train_pred, y_train)
      #3. zero out gradients
      optimizer.zero_grad()
      #4. perform backprop
      loss.backward()
      #5. update weights
      optimizer.step()

      print("Epoch ", t, "MSE: ", loss.item())
      train_losses[t] = loss.item()
  training_time = time.time()-start_time
  print("Training time: {}".format(training_time))
  return y_train_pred, train_losses, training_time

rnn_train_pred, train_losses, training_time =train(rnn, x_train, y_train,num_epochs=100)

predict = pd.DataFrame(scaler.inverse_transform(rnn_train_pred.detach().numpy()))
original = pd.DataFrame(scaler.inverse_transform(y_train.detach().numpy()))

"""### Visualize the prediction results on train data"""

def plot_training_results(predict, original, train_losses, model_name):
  sns.set_style("darkgrid")
  fig = plt.figure()
  fig.subplots_adjust(hspace=0.2, wspace=0.2)
  plt.subplot(1, 2, 1)
  ax = sns.lineplot(x = original.index, y = original[0], label="Data", color='royalblue')
  ax = sns.lineplot(x = predict.index, y = predict[0], label="Training Prediction (" + model_name +")" , color='tomato')
  ax.set_title(stock_ticker+' Stock price', size = 14, fontweight='bold')
  ax.set_xlabel("Days", size = 14)
  ax.set_ylabel("Cost (USD)", size = 14)
  ax.set_xticklabels('', size=10)

  plt.subplot(1, 2, 2)
  ax = sns.lineplot(data=train_losses, color='royalblue')
  ax.set_xlabel("Epoch", size = 14)
  ax.set_ylabel("Loss", size = 14)
  ax.set_title("Training Loss", size = 14, fontweight='bold')
  fig.set_figheight(6)
  fig.set_figwidth(16)

plot_training_results(predict, original, train_losses, 'RNN')

import math, time
from sklearn.metrics import mean_squared_error

def evaluation(model,x_test,y_test,y_train,y_train_pred):
  result=[]

  # initialize the hidden state
  hidden = None
  y_test_pred, hidden = model(x_test, hidden)

  # invert predictions
  y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())
  y_test_orig = scaler.inverse_transform(y_test.detach().numpy())
  y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())
  y_train_orig = scaler.inverse_transform(y_train.detach().numpy())
  # calculate root mean squared error
  trainScore = math.sqrt(mean_squared_error(y_train_orig[:,0], y_train_pred[:,0]))
  print('Train Score: %.2f RMSE' % (trainScore))
  testScore = math.sqrt(mean_squared_error(y_test_orig[:,0], y_test_pred[:,0]))
  print('Test Score: %.2f RMSE' % (testScore))
  result.append(trainScore)
  result.append(testScore)
  result.append(training_time)
  return y_test_pred, result

rnn_test_pred, rnn_result=evaluation(rnn,x_test,y_test,y_train,rnn_train_pred)

"""#plot the predictions on train data, test data, and the actual values."""

import plotly.express as px
import plotly.graph_objects as go

def plot_predictions(y_train_pred,y_test_pred, model_name):
  y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())
  # shift train predictions for plotting
  trainPredictPlot = np.empty_like(price)
  trainPredictPlot[:, :] = np.nan
  trainPredictPlot[lookback:len(y_train_pred)+lookback, :] = y_train_pred

  # shift test predictions for plotting
  testPredictPlot = np.empty_like(price)
  testPredictPlot[:, :] = np.nan
  testPredictPlot[len(y_train_pred)+lookback-1:len(price)-1, :] = y_test_pred

  original = scaler.inverse_transform(price['Close'].values.reshape(-1,1))
  predictions = np.append(trainPredictPlot, testPredictPlot, axis=1)
  predictions = np.append(predictions, original, axis=1)
  result = pd.DataFrame(predictions)

  fig = go.Figure()
  fig.add_trace(go.Scatter(go.Scatter(x=data['Date'], y=result[0],
                    mode='lines',
                    name='Train prediction')))
  fig.add_trace(go.Scatter(x=data['Date'], y=result[1],
                    mode='lines',
                    name='Test prediction'))
  fig.add_trace(go.Scatter(go.Scatter(x=data['Date'], y=result[2],
                    mode='lines',
                    name='Actual Value')))
  fig.update_layout(
    xaxis=dict(
        showline=True,
        showgrid=True,
        showticklabels=True,
        linecolor='white',
        linewidth=2
    ),
    yaxis=dict(
        title_text='Close (USD)',
        titlefont=dict(
            family='Rockwell',
            size=12,
            color='white',
        ),
        showline=True,
        showgrid=True,
        showticklabels=True,
        linecolor='white',
        linewidth=2,
        ticks='outside',
        tickfont=dict(
            family='Rockwell',
            size=12,
            color='white',
        ),
    ),
    showlegend=True,
    template = 'plotly_dark'

  )

  annotations = []
  annotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05,
                              xanchor='left', yanchor='bottom',
                              text='Results ('+ model_name + ')',
                              font=dict(family='Rockwell',
                                        size=26,
                                        color='white'),
                              showarrow=False))
  fig.update_layout(annotations=annotations)

  fig.show()

plot_predictions(rnn_train_pred,rnn_test_pred,'RNN')

"""## Define LSTM Model"""

class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        """
        Initialize the model by setting up the layers.
        """
        super(LSTM, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        """
        Perform a forward pass of our model on some input and hidden state.
        """
        # x (batch_size, seq_length, input_dim)
        # hidden (num_layers, batch_size, hidden_dim)
        # r_out (batch_size, time_step, hidden_size)

        # Here we initialize the hidden states
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()  #short term memory
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()  #long term memory

        ## Get the outputs and the new hidden state from the lstm
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        ## Put out through the fully-connected layer
        out = self.fc(out[:, -1, :])
        return out

lstm = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(lstm.parameters(), lr=0.01)

def train(model, x_train, y_train, num_epochs):
    train_losses = np.zeros(num_epochs)
    start_time = time.time()
    for t in range(num_epochs):
      y_train_pred = model(x_train)
      loss = criterion(y_train_pred, y_train)
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
      print("Epoch ", t, "MSE: ", loss.item())
      train_losses[t] = loss.item()

    training_time = time.time()-start_time
    print("Training time: {}".format(training_time))
    return y_train_pred, train_losses,training_time

lstm_train_pred, train_losses,training_time=train(lstm, x_train, y_train, num_epochs=100)

predict = pd.DataFrame(scaler.inverse_transform(lstm_train_pred.detach().numpy()))

plot_training_results(predict, original, train_losses, 'LSTM')

"""Having finished the training, we can apply the model to predict prices on the test data."""

def evaluation(model,x_test,y_test,y_train,y_train_pred):
  result=[]
  y_test_pred = model(x_test)
  # invert predictions
  y_test_pred = scaler.inverse_transform(y_test_pred.detach().numpy())
  y_test_orig = scaler.inverse_transform(y_test.detach().numpy())
  y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())
  y_train_orig = scaler.inverse_transform(y_train.detach().numpy())
  # calculate root mean squared error
  trainScore = math.sqrt(mean_squared_error(y_train_orig[:,0], y_train_pred[:,0]))
  print('Train Score: %.2f RMSE' % (trainScore))
  testScore = math.sqrt(mean_squared_error(y_test_orig[:,0], y_test_pred[:,0]))
  print('Test Score: %.2f RMSE' % (testScore))
  result.append(trainScore)
  result.append(testScore)
  result.append(training_time)
  return y_test_pred, result

lstm_test_pred, lstm_result=evaluation(lstm,x_test,y_test,y_train,lstm_train_pred)

plot_predictions(lstm_train_pred,lstm_test_pred,'LSTM')

"""## Define GRU model"""

class GRU(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(GRU, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers

        # Define the GRU model layer
        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()
        out, (hn) = self.gru(x, (h0.detach()))
        out = self.fc(out[:, -1, :])
        return out

gru = GRU(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
criterion = torch.nn.MSELoss()
optimizer = torch.optim.Adam(gru.parameters(), lr=0.01)

gru_train_pred, train_losses,training_time=train(gru, x_train, y_train, num_epochs=100)

predict = pd.DataFrame(scaler.inverse_transform(gru_train_pred.detach().numpy()))

plot_training_results(predict, original, train_losses, 'GRU')

gru_test_pred, gru_result=evaluation(gru,x_test,y_test,y_train,gru_train_pred)

plot_predictions(gru_train_pred,gru_test_pred,'GRU')

"""## Conclusion:

All models have good performances in the training phase. The GRU neural network outperformed both the basic RNN and the LSTM in terms of precision because it reached a lower mean square error (in training, and most importantly, in the test set) and in speed, seen that RNN is the fastest, GRU is the second, and LSTM is the slowest.

Note that here the LSTM does not perform well in predicting stock prices. I guess the potential reason is that there are not much long term memory/correlation among stock prices over time. Also, our sequence length is not that long and only spans 20 days.
"""

rnn_result_df=pd.DataFrame(rnn_result, columns=['RNN'])
lstm_result_df = pd.DataFrame(lstm_result, columns=['LSTM'])
gru_result_df = pd.DataFrame(gru_result, columns=['GRU'])
result = pd.concat([rnn_result_df,lstm_result_df, gru_result_df], axis=1, join='inner')
result.index = ['Train RMSE', 'Test RMSE', 'Train Time']
result